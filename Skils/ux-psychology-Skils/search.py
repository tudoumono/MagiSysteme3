#!/usr/bin/env python3
"""
UX Psychology Concept Search Script

Usage:
    python3 search.py "<query>" [--category <category>] [-n <max_results>]

Examples:
    python3 search.py "ä¾¡æ ¼ ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³"
    python3 search.py "ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°" --category onboarding
    python3 search.py "ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•" -n 10
"""

import csv
import sys
import argparse
import re
from pathlib import Path
from collections import defaultdict


# ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
CATEGORY_MAP = {
    'pricing': ['pricing'],
    'conversion': ['conversion'],
    'onboarding': ['onboarding'],
    'cognitive': ['cognitive'],
    'engagement': ['engagement'],
    'visual': ['visual'],
    'bias': ['bias'],
    'all': ['pricing', 'conversion', 'onboarding', 'cognitive', 'engagement', 'visual', 'bias']
}


def load_concepts(data_path: Path) -> list:
    """CSVã‹ã‚‰ã‚³ãƒ³ã‚»ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    concepts = []
    csv_file = data_path / 'concepts.csv'
    
    if not csv_file.exists():
        print(f"Error: Data file not found: {csv_file}", file=sys.stderr)
        sys.exit(1)
    
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            concepts.append(row)
    
    return concepts


def tokenize(text: str) -> set:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰"""
    # è‹±æ•°å­—ã¨æ—¥æœ¬èªã®ä¸¡æ–¹ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³æŠ½å‡º
    text = text.lower()
    # ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚«ãƒ³ãƒã€å¥èª­ç‚¹ã§åˆ†å‰²
    tokens = re.split(r'[\s,ã€ã€‚]+', text)
    # ç©ºæ–‡å­—ã‚’é™¤å»
    return set(t for t in tokens if t)


def calculate_score(concept: dict, query_tokens: set) -> float:
    """æ¤œç´¢ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 0.0
    
    # æ¤œç´¢å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨é‡ã¿
    fields = {
        'name_ja': 5.0,
        'name_en': 5.0,
        'keywords': 3.0,
        'definition': 2.0,
        'why_it_works': 1.5,
        'example': 1.0
    }
    
    for field, weight in fields.items():
        field_text = concept.get(field, '').lower()
        field_tokens = tokenize(field_text)
        
        for qt in query_tokens:
            # å®Œå…¨ä¸€è‡´
            if qt in field_tokens:
                score += weight * 2
            # éƒ¨åˆ†ä¸€è‡´
            elif any(qt in ft or ft in qt for ft in field_tokens):
                score += weight
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å†…ã«å«ã¾ã‚Œã‚‹
            elif qt in field_text:
                score += weight * 0.5
    
    return score


def search(concepts: list, query: str, category: str = 'all', max_results: int = 5) -> list:
    """ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’æ¤œç´¢"""
    query_tokens = tokenize(query)
    
    if not query_tokens:
        return []
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
    target_categories = CATEGORY_MAP.get(category, CATEGORY_MAP['all'])
    filtered_concepts = [c for c in concepts if c.get('category') in target_categories]
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    scored = []
    for concept in filtered_concepts:
        score = calculate_score(concept, query_tokens)
        if score > 0:
            scored.append((concept, score))
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    scored.sort(key=lambda x: x[1], reverse=True)
    
    return scored[:max_results]


def format_result(concept: dict, score: float, rank: int) -> str:
    """æ¤œç´¢çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    output = []
    output.append(f"\n{'='*60}")
    output.append(f"#{rank} {concept['name_ja']} ({concept['name_en']})")
    output.append(f"ã‚«ãƒ†ã‚´ãƒª: {concept['category']} | ã‚¹ã‚³ã‚¢: {score:.1f}")
    output.append(f"{'='*60}")
    output.append(f"\nã€å®šç¾©ã€‘\n{concept['definition']}")
    output.append(f"\nã€ãªãœæ©Ÿèƒ½ã™ã‚‹ã‹ã€‘\n{concept['why_it_works']}")
    output.append(f"\nã€å®Ÿä¾‹ã€‘\n{concept['example']}")
    output.append(f"\nã€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘\n{concept['keywords']}")
    
    return '\n'.join(output)


def main():
    parser = argparse.ArgumentParser(description='UXå¿ƒç†å­¦ã‚³ãƒ³ã‚»ãƒ—ãƒˆæ¤œç´¢')
    parser.add_argument('query', help='æ¤œç´¢ã‚¯ã‚¨ãƒª')
    parser.add_argument('--category', '-c', default='all',
                        choices=['pricing', 'conversion', 'onboarding', 'cognitive', 
                                'engagement', 'visual', 'bias', 'all'],
                        help='ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿')
    parser.add_argument('-n', '--max-results', type=int, default=5,
                        help='æœ€å¤§çµæœæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)')
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/dataï¼‰
    script_dir = Path(__file__).parent.resolve()
    data_path = script_dir.parent / 'data'
    
    # ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿
    concepts = load_concepts(data_path)
    
    # æ¤œç´¢å®Ÿè¡Œ
    results = search(concepts, args.query, args.category, args.max_results)
    
    if not results:
        print(f"\næ¤œç´¢çµæœãªã—: '{args.query}'")
        print("åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚")
        return
    
    print(f"\nğŸ” æ¤œç´¢: '{args.query}' (ã‚«ãƒ†ã‚´ãƒª: {args.category})")
    print(f"ğŸ“Š {len(results)}ä»¶ã®çµæœ")
    
    for i, (concept, score) in enumerate(results, 1):
        print(format_result(concept, score, i))
    
    print(f"\n{'='*60}")
    print("ğŸ’¡ æ´»ç”¨ã®ãƒ’ãƒ³ãƒˆ:")
    print("- è¤‡æ•°ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’çµ„ã¿åˆã‚ã›ã¦UI/UXã‚’è¨­è¨ˆ")
    print("- ã€Œãªãœæ©Ÿèƒ½ã™ã‚‹ã‹ã€ã‚’ç†è§£ã—ã¦é©åˆ‡ã«é©ç”¨")
    print("- A/Bãƒ†ã‚¹ãƒˆã§åŠ¹æœã‚’æ¤œè¨¼")


if __name__ == '__main__':
    main()
